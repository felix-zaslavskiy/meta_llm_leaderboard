# meta_llm_leaderboard
Meta leaderboard combines data from multiple different leaderboards about LLMs.
The focus is primarily on Open LLMs.

Todo:
- Add to automated change tracker for AgentBench
- Track the code benchmark bookmarked in X
- Create visualizations for top models for other benchmarks by model size. https://huggingface.co/spaces/bigcode/multilingual-code-evals

Ideas:
- Create across everything chart. Show model that has been evaluated on all leaderboards.
- Add a runner-up (commercial permissible) license to HF best models.
- Work on Supernatural Instruction
- Take another look at HELM (They hardly evaluate recent open models. Not even bother with finetunes)
- https://gpt4all.io/index.html is that Test set worth it?
- https://github.com/defog-ai/sqlcoder sqlcode.. not a leaderboard but nice evaluation